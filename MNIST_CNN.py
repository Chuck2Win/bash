# -*- coding: utf-8 -*-
"""Untitled41.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vs7fHjDczBv77545vd_0MNtUuD-xj9fZ
"""

from torchvision.datasets import MNIST
import torchvision.transforms as transforms
# DataLoader
from torch.utils.data import DataLoader
import torch 
import torch.nn as nn
from sklearn.metrics import classification_report
import argparse
import os
import logging
from tqdm import tqdm

parser = argparse.ArgumentParser(description = '필요한 변수')
parser.add_argument('--channels', default = 1, type=int)
parser.add_argument('--out_channels', default = 32, type=int)
parser.add_argument('--kernel_size', default = 3, type=int)
parser.add_argument('--stride', default = 1, type=int)
parser.add_argument('--padding', default = 1, type=int)
parser.add_argument('--batch_size', default = 128, type=int)
parser.add_argument('--n_labels', default = 10, type=int)
parser.add_argument('--input_dim', default = 28, type=int)
parser.add_argument('--epochs', default = 32, type=int)
parser.add_argument('--logging_path', default = '/content/drive/MyDrive/LOGS')
args = vars(parser.parse_args())

args['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'
    
"""# Data Download"""

download_root='./MNIST_DATASET'
train_dataset = MNIST(download_root, transform = transforms.ToTensor(), train=True, download=True)
valid_dataset = MNIST(download_root, transform = transforms.ToTensor(), train=False, download=True)
train_dataloader = DataLoader(train_dataset, batch_size = args['batch_size'], shuffle=True)
val_dataloader = DataLoader(valid_dataset, batch_size = args['batch_size'], shuffle=True)

"""# Logger"""

# logging : 선언
logger = logging.getLogger()
logging.basicConfig(format='%(asctime)s - %(message)s', # format
                    datefmt='%Y-%m-%d %H:%M:%S', 
                    level=logging.INFO, # level
                    handlers=[logging.FileHandler(os.path.join(args['logging_path'],'Log.log'))]) # stream or file(<- format도 됨.)

class CNNClassifier(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.cnn = nn.Sequential(nn.Conv2d(args['channels'], args['out_channels'], args['kernel_size'], args['stride'], args['padding']),nn.ReLU(),nn.MaxPool2d(args['kernel_size'],args['stride'],args['padding']))
        self.fc=nn.Sequential(nn.Linear(args['out_channels']*(args['input_dim']**2),args['out_channels']*(args['input_dim']**2)//100),nn.ReLU(),nn.Linear(args['out_channels']*(args['input_dim']**2)//100, args['n_labels']))
    
    def forward(self,data):
        output = self.cnn.forward(data)
        bs = output.size(0)
        output = output.reshape(bs,-1).contiguous()
        output = self.fc.forward(output)
        return output

def train(args, model, dataloader, optimizer,criterion):
    model.train()
    Loss = 0
    for epoch in tqdm(range(1,1+args['epochs']),desc = 'epoch'):
        for data in dataloader:
            optimizer.zero_grad()
            data,label = data
            data = data.to(args['device'])
            label = label.to(args['device'])
            output = model.forward(data)
            loss = criterion(output,label)
            Loss+=loss.item()
            loss.backward()
            optimizer.step()
        Loss = Loss/len(dataloader)
        logger.info('='*100)
        logger.info(f"epoch : {epoch}, loss : {Loss}")
        with torch.no_grad():
            model.eval()
            y_true = []
            y_pred = []
            for data in dataloader:
                data,label = data
                data = data.to(args['device'])
                y_true.extend(label.tolist())
                output = model.forward(data)
                y_pred.extend(output.argmax(-1).tolist())
            logger.info(classification_report(y_true, y_pred, labels = list(range(0,10))))
            logger.info('='*100)
    logger.info("training end")
if __name__=='__main__':
    #print(args)
    # model = CNNClassifier(args).to(args['device'])
    optimizer = torch.optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()
    train(args,model,train_dataloader,optimizer,criterion)
